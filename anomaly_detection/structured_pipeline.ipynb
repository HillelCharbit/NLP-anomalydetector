{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62abe918",
   "metadata": {},
   "source": [
    "\n",
    "# Stock Market Prediction Pipeline\n",
    "\n",
    "- Load and preprocess stock data.\n",
    "- Apply various anomaly detection models.\n",
    "- Extract features and use Chronos for embeddings.\n",
    "- Make time series predictions.\n",
    "- Compare results against actual stock prices.\n",
    "\n",
    "## Pipeline Overview\n",
    "1. **Data Ingestion & Preprocessing**\n",
    "2. **Anomaly Detection (SOTA Models)**\n",
    "3. **Feature Engineering (Chronos Embeddings & Indicators)**\n",
    "4. **Modeling & Prediction (Chronos for Time Series)**\n",
    "5. **Evaluation & Comparison (Backtesting & Visualization)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f415ff61",
   "metadata": {},
   "source": [
    "## 1. Data Ingestion & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2793dcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the directories\n",
    "directories = [\"stock_data\", \"stock_predictions\"]\n",
    "\n",
    "# Create directories if they do not exist\n",
    "for directory in directories:\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f8a80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mariomattiasulmonte/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, tickers, start_date, end_date):\n",
    "        self.tickers = tickers\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "\n",
    "    def load_data(self):\n",
    "        data = {}\n",
    "        for ticker in self.tickers:\n",
    "            df = yf.download(ticker, start=self.start_date, end=self.end_date)[[\"Close\"]].copy()\n",
    "            \n",
    "            # RSI implementation\n",
    "            window = 14\n",
    "            delta = df[\"Close\"].diff()\n",
    "            gain = (delta.where(delta > 0, 0)).copy()\n",
    "            loss = (-delta.where(delta < 0, 0)).copy()\n",
    "            avg_gain = gain.rolling(window=window).mean()\n",
    "            avg_loss = loss.rolling(window=window).mean()\n",
    "            rs = avg_gain / avg_loss\n",
    "            df[\"RSI\"] = 100 - (100 / (1 + rs))\n",
    "            \n",
    "            # RSI cross signal\n",
    "            df[\"Indicator\"] = np.where((df[\"RSI\"].shift(1) > 30) & (df[\"RSI\"] < 30), 1, 0)\n",
    "            \n",
    "            # drop na values\n",
    "            df.dropna(inplace=True)\n",
    "            \n",
    "            data[ticker] = df\n",
    "        return data\n",
    "\n",
    "# Example usage\n",
    "tickers = [\"AAPL\", \"MSFT\", \"GOOGL\", \"KO\", \"AMZN\"]\n",
    "data_loader = DataLoader(tickers, \"2010-01-01\", \"2025-01-01\")\n",
    "stock_data = data_loader.load_data()\n",
    "\n",
    "for ticker, df in stock_data.items():\n",
    "    path = \"stock_data\"\n",
    "    df.to_csv(f\"{path}/{ticker}.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4196763b",
   "metadata": {},
   "source": [
    "## 2. Chronos Forcasting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0244cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chronos import ChronosPipeline\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "# fetch data from csv folder function \n",
    "def load_data(ticker):\n",
    "    path = \"stock_data\"\n",
    "    df = pd.read_csv(f\"{path}/{ticker}.csv\", index_col=0, parse_dates=True)\n",
    "    return df\n",
    "  \n",
    "# Split data into train and test sets\n",
    "def train_test_split(df, test_size=64):\n",
    "    train_data = {}\n",
    "    test_data = {}\n",
    "    \n",
    "    train_data = df.iloc[:-test_size]\n",
    "    test_data = df.iloc[-test_size:]\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "def predict(ticker, train_data, test_data, pipeline):\n",
    "\n",
    "    # Convert training data to tensor\n",
    "    context = torch.tensor(train_data[\"Close\"].values, dtype=torch.bfloat16)\n",
    "    prediction_length = len(test_data)\n",
    "\n",
    "    # Predict\n",
    "    output = pipeline.predict(\n",
    "        context=context,\n",
    "        prediction_length=prediction_length,\n",
    "        limit_prediction_length=False,\n",
    "    )\n",
    "\n",
    "    # Convert output tensor to DataFrame\n",
    "    output_reshaped = output.cpu().numpy().flatten()\n",
    "    future_dates = pd.date_range(start=test_data.index[0], periods=prediction_length, freq='B')\n",
    "\n",
    "    # Ensure all arrays are of the same length\n",
    "    min_length = min(len(future_dates), len(test_data[\"Close\"].values), len(output_reshaped))\n",
    "    future_dates = future_dates[:min_length]\n",
    "    actual_close = test_data[\"Close\"].values[:min_length]\n",
    "    predicted_close = output_reshaped[:min_length]\n",
    "\n",
    "    # Create a DataFrame with actual and predicted values\n",
    "    predictions_df = pd.DataFrame({\n",
    "        \"Date\": future_dates,\n",
    "        \"Actual_Close\": actual_close,\n",
    "        \"Predicted_Close\": predicted_close,\n",
    "        \"Indicator\": test_data[\"Indicator\"].values[:min_length]\n",
    "    })\n",
    "    output_dir = \"stock_predictions\"\n",
    "    \n",
    "    # Save to CSV\n",
    "    file_path = f\"{output_dir}/{ticker}_predictions.csv\"\n",
    "    predictions_df.to_csv(file_path, index=False)\n",
    "    print(f\"Saved predictions for {ticker} to {file_path}\")\n",
    "     \n",
    "# Use CUDA if available, otherwise CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create Chronos pipeline\n",
    "pipeline = ChronosPipeline.from_pretrained(\n",
    "    \"amazon/chronos-t5-base\",\n",
    "    device_map=device,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "# automize for all tickers\n",
    "tickers = [\"AAPL\", \"MSFT\", \"GOOGL\", \"KO\", \"AMZN\"]\n",
    "\n",
    "for ticker in tickers:\n",
    "    stock_df = load_data(ticker)\n",
    "    train_data, test_data = train_test_split(stock_df)\n",
    "    predict(ticker, train_data, test_data, pipeline)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb6751e",
   "metadata": {},
   "source": [
    "## 3. Anomaly prediction using time intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bde46af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions for AAPL to stock_predictions/AAPL_predictions.csv\n",
      "Saved predictions for MSFT to stock_predictions/MSFT_predictions.csv\n",
      "Saved predictions for GOOGL to stock_predictions/GOOGL_predictions.csv\n",
      "Saved predictions for KO to stock_predictions/KO_predictions.csv\n",
      "Saved predictions for AMZN to stock_predictions/AMZN_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "def predict_anomalies(ticker, train_data, test_data, pipeline, threshold=0.1, device=None):\n",
    "    df = train_data.copy()\n",
    "    df[\"Close\"] = df[\"Close\"]*df[\"Indicator\"]\n",
    "    # Convert training data to tensor\n",
    "    context = torch.tensor(df[\"Close\"].values, dtype=torch.bfloat16)\n",
    "    prediction_length = len(test_data)\n",
    "\n",
    "    # Predict\n",
    "    output = pipeline.predict(\n",
    "        context=context,\n",
    "        prediction_length=prediction_length,\n",
    "        limit_prediction_length=False,\n",
    "    )\n",
    "\n",
    "    # Convert output tensor to DataFrame\n",
    "    output_reshaped = output.cpu().numpy().flatten()\n",
    "    future_dates = pd.date_range(start=test_data.index[0], periods=prediction_length, freq='B')\n",
    "\n",
    "    # Ensure all arrays are of the same length\n",
    "    min_length = min(len(future_dates), len(test_data[\"Close\"].values), len(output_reshaped))\n",
    "    predicted_anomaly = output_reshaped[:min_length]\n",
    "    # read the predictions\n",
    "    predictions_df = pd.read_csv(f\"stock_predictions/{ticker}_predictions.csv\")\n",
    "    # add the anomaly predictions to the dataframe, if the predicted value is greater than a threshold, then it is an anomaly\n",
    "    predictions_df[\"Chronos_prediction\"] = np.where(predicted_anomaly > threshold, 1, 0)\n",
    "    \n",
    "    output_dir = \"stock_predictions\"\n",
    "    # Save to CSV\n",
    "    file_path = f\"{output_dir}/{ticker}_predictions.csv\"\n",
    "    predictions_df.to_csv(file_path, index=False)\n",
    "    print(f\"Saved predictions for {ticker} to {file_path}\")\n",
    "     \n",
    "# Use CUDA if available, otherwise CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "for ticker in tickers:\n",
    "    stock_df = load_data(ticker)\n",
    "    train_data, test_data = train_test_split(stock_df)\n",
    "    predict_anomalies(ticker, train_data, test_data, pipeline, threshold=0.1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b65d8da",
   "metadata": {},
   "source": [
    "## 4. Anomaly Detection using the SOTA on predicted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7100fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions for AAPL to stock_predictions/AAPL_predictions.csv\n",
      "Saved predictions for MSFT to stock_predictions/MSFT_predictions.csv\n",
      "Saved predictions for GOOGL to stock_predictions/GOOGL_predictions.csv\n",
      "Saved predictions for KO to stock_predictions/KO_predictions.csv\n",
      "Saved predictions for AMZN to stock_predictions/AMZN_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.iforest import IForest\n",
    "\n",
    "def sota_predict(model, ticker, device=None):\n",
    "    # get data from stock predictions\n",
    "    path = \"stock_predictions\"\n",
    "    stock_predictions = pd.read_csv(f\"{path}/{ticker}_predictions.csv\")\n",
    "    \n",
    "    model.fit(stock_predictions[\"Predicted_Close\"].values.reshape(-1, 1))\n",
    "    \n",
    "    # Predict anomalies\n",
    "    stock_predictions[\"SOTA_prediction\"] = model.predict(stock_predictions[\"Predicted_Close\"].values.reshape(-1, 1))\n",
    "    \n",
    "    # Save to CSV\n",
    "    output_dir = \"stock_predictions\"\n",
    "    file_path = f\"{output_dir}/{ticker}_predictions.csv\"\n",
    "    stock_predictions.to_csv(file_path, index=False)\n",
    "    print(f\"Saved predictions for {ticker} to {file_path}\")\n",
    "    \n",
    "# Create IForest model\n",
    "model = IForest(n_estimators=100, random_state=42)\n",
    "\n",
    "for ticker in tickers:\n",
    "    sota_predict(model, ticker,device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f845e1e",
   "metadata": {},
   "source": [
    "## 5 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b63cf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "DIRECTORY_PATH = \"stock_predictions\"\n",
    "\n",
    "# List all CSV files in the directory\n",
    "csv_files = glob.glob(os.path.join(DIRECTORY_PATH, \"*.csv\"))\n",
    "\n",
    "# A list to store results from each file\n",
    "results = []\n",
    "\n",
    "def compute_weekly_classification_metrics(true_mask, pred_mask):\n",
    "    \"\"\"\n",
    "    Returns (accuracy, recall, precision, f1).\n",
    "    \"\"\"\n",
    "    # Basic counts\n",
    "    tp = (true_mask & pred_mask).sum()\n",
    "    tn = (~true_mask & ~pred_mask).sum()\n",
    "    fp = (~true_mask & pred_mask).sum()\n",
    "    fn = (true_mask & ~pred_mask).sum()\n",
    "\n",
    "    # Numerators/denominators\n",
    "    recall = tp / (tp + fn) if (tp + fn) else np.nan\n",
    "    precision = tp / (tp + fp) if (tp + fp) else np.nan\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision and recall) else np.nan\n",
    "\n",
    "    return f1\n",
    "\n",
    "def evaluate_predictor(weekly_sums, true_col, pred_col):\n",
    "    \"\"\"\n",
    "    Evaluates a single predictor and returns its metrics.\n",
    "    \"\"\"\n",
    "    true_mask = weekly_sums[true_col] > 0\n",
    "    pred_mask = weekly_sums[pred_col] > 0\n",
    "\n",
    "    f1 = compute_weekly_classification_metrics(true_mask, pred_mask)\n",
    "\n",
    "    exact_match_rate = (weekly_sums[true_col] == weekly_sums[pred_col]).mean()\n",
    "\n",
    "    mae = mean_absolute_error(weekly_sums[true_col], weekly_sums[pred_col])\n",
    "\n",
    "    # Calculate FPR\n",
    "    neg_mask = ~true_mask  # Weeks without anomalies\n",
    "    fp_mask = neg_mask & pred_mask  # False positive predictions\n",
    "    false_positives = fp_mask.sum()\n",
    "    true_negatives_total = neg_mask.sum()\n",
    "\n",
    "    fpr = false_positives / true_negatives_total if true_negatives_total > 0 else np.nan\n",
    "\n",
    "    return {\n",
    "        'f1': f1,\n",
    "        'exact_match_rate': exact_match_rate,\n",
    "        'mae': mae,\n",
    "        'fpr': fpr\n",
    "    }\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    df = pd.read_csv(csv_file)\n",
    "    # Convert 'Date' to datetime if not already\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "    # Group by non-overlapping weekly periods\n",
    "    df['week'] = df['Date'].dt.to_period('W')\n",
    "\n",
    "    # Sum columns within each week\n",
    "    weekly_sums = df.groupby('week', as_index=False).agg({\n",
    "        'Indicator': 'sum',           # True number of positives\n",
    "        'Chronos_prediction': 'sum', # Model 1 predicted positives\n",
    "        'SOTA_prediction': 'sum',        # Model 2 (SOTA) predicted positives\n",
    "    })\n",
    "\n",
    "    # Evaluate each predictor\n",
    "    file_results = {'file': os.path.basename(csv_file)}\n",
    "    for predictor in ['Chronos_prediction', 'SOTA_prediction']:\n",
    "        metrics = evaluate_predictor(weekly_sums, 'Indicator', predictor)\n",
    "        file_results.update({f'{k}_{predictor.split(\"_\")[0]}': v for k, v in metrics.items()})  # Rename keys\n",
    "\n",
    "    results.append(file_results)\n",
    "\n",
    "# Convert all results to a single DataFrame for easy comparison\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('evaluation.csv')\n",
    "print(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
